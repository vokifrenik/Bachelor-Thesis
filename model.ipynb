{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "\n",
    "from PIL import Image           # Image management\n",
    "import numpy as np              # Numerical computing\n",
    "import matplotlib.pyplot as plt # Plotting\n",
    "\n",
    "from nes_py.wrappers import JoypadSpace     # Joypad wrapper in NES Emulator\n",
    "\n",
    "import gym_super_mario_bros                                 # Super Mario environment\n",
    "from gym_super_mario_bros.actions import SIMPLE_MOVEMENT    # Import the simplified controls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create environment - Note there are different environments, for more info: https://pypi.org/project/gym-super-mario-bros/\n",
    "env = gym_super_mario_bros.make(\"SuperMarioBros-v2\")\n",
    "\n",
    "# Setup simplified controls\n",
    "env = JoypadSpace(env, SIMPLE_MOVEMENT)\n",
    "\n",
    "# actions for simple movement\n",
    "# SIMPLE_MOVEMENT = [\n",
    "#     ['NOOP'],\n",
    "#     ['right'],\n",
    "#     ['right', 'A'],\n",
    "#     ['right', 'B'],\n",
    "#     ['right', 'A', 'B'],\n",
    "#     ['A'],\n",
    "#     ['left'],\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us explore the space and action space in this environment\n",
    "\n",
    "# Reset environment\n",
    "env.reset()\n",
    "\n",
    "# Sample one random action\n",
    "action = env.action_space.sample()\n",
    "\n",
    "# One agent-environment iteration \n",
    "next_state, reward, done, info = env.step(action)\n",
    "\n",
    "# Print results\n",
    "print('The shape of the states is {}'.format(next_state.shape))\n",
    "\n",
    "print('The reward for this iteration is {}'.format(reward))\n",
    "\n",
    "print('The done flag in this iteration is {}'.format(done))\n",
    "\n",
    "print('Here is more information about this iteration {}'.format(info))\n",
    "\n",
    "\n",
    "# Plot next state\n",
    "plt.figure(figsize=(20, 16))\n",
    "plt.imshow(next_state)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Agent Loop\n",
    "\n",
    "# Done flag - Termination of episode\n",
    "done = True\n",
    "\n",
    "# Number of iterations\n",
    "num_iterations = 100\n",
    "\n",
    "# Iteration loop\n",
    "for steps in range(num_iterations):\n",
    "    \n",
    "    # Check if the episode is done\n",
    "    if done:\n",
    "        env.reset()\n",
    "    \n",
    "    # Sample one random action\n",
    "    action = env.action_space.sample()\n",
    "    \n",
    "    # agent-environment iteration \n",
    "    next_state, reward, done, info = env.step(action)\n",
    "    \n",
    "    # Render may crash Jupyter\n",
    "    env.render()\n",
    "    \n",
    "    # Check for new reward\n",
    "    print('The reward in step {} is {}'.format(steps, reward))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close the environment        \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrappers to vectorize and stack frames\n",
    "from stable_baselines3.common.vec_env import VecFrameStack, DummyVecEnv\n",
    "\n",
    "# RL algorithms in the library\n",
    "from stable_baselines3 import DQN, A2C, DDPG, PPO, SAC, TD3\n",
    "\n",
    "# Import Frame Stacker Wrapper and GrayScaling Wrapper\n",
    "from gym.wrappers import GrayScaleObservation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the wrappers\n",
    "\n",
    "# Grayscale\n",
    "env = GrayScaleObservation(env, keep_dim=True)\n",
    "\n",
    "# Vectorize environment - call the environment in sequence on the current Python process\n",
    "env =  DummyVecEnv([lambda: env])\n",
    "\n",
    "# Stack frames\n",
    "env = VecFrameStack(env, n_stack = 4, channels_order='last')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset environment\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample one random action\n",
    "action = env.action_space.sample()\n",
    "\n",
    "# One agent-environment iteration \n",
    "next_state, reward, done, info = env.step([5])\n",
    "\n",
    "# Print results\n",
    "print('The shape of the states is {}'.format(next_state.shape))\n",
    "\n",
    "print('The reward for this iteration is {}'.format(reward))\n",
    "\n",
    "print('The done flag in this iteration is {}'.format(done))\n",
    "\n",
    "print('Here is more information about this iteration {}'.format(info))\n",
    "\n",
    "\n",
    "# Visualize the new state, i.e, past 4 frames\n",
    "plt.figure(figsize=(20,16))\n",
    "for idx in range(next_state.shape[3]):\n",
    "    plt.subplot(1,4,idx+1)\n",
    "    plt.imshow(next_state[0][:,:,idx])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train a model\n",
    "model = PPO(policy = 'CnnPolicy', env = env, verbose = 1)\n",
    "model.learn(total_timesteps = 1000)\n",
    "\n",
    "\n",
    "# Start the game \n",
    "state = env.reset()\n",
    "# Loop through the game\n",
    "while True: \n",
    "    \n",
    "    action, _ = model.predict(state)\n",
    "    state, reward, done, info = env.step(action)\n",
    "    env.render()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_environment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
